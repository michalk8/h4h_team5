{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import pickle\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from vis.utils import utils\n",
    "import numpy as np\n",
    "# from scipy.misc import imsave\n",
    "import imageio\n",
    "# imageio.imwrite('filename.jpg', array)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 1\n",
    "\n",
    "img_height = 400\n",
    "img_width = 400\n",
    "img_channels = 3\n",
    "\n",
    "img_height_target = 200\n",
    "img_width_target = 200\n",
    "target_size = (img_height_target, img_width_target)\n",
    "\n",
    "cardinality = 32\n",
    "\n",
    "train_dir = r'train'\n",
    "validation_dir = r'validation'\n",
    "\n",
    "total_train = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "total_val = sum([len(files) for r, d, files in os.walk(validation_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ResNEXT Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "        precision = precision_m(y_true, y_pred)\n",
    "        recall = recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Jupyter/2019a-gcccoremkl-8.3.0-2019.3.199-Python-3.6.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-c660a655bb9b>\", line 110, in <module>\n",
      "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 442, in _method_wrapper\n",
      "    method(self, *args, **kwargs)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 499, in compile\n",
      "    sample_weights=self.sample_weights)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1844, in _handle_metrics\n",
      "    return_stateful_result=return_stateful_result))\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1800, in _handle_per_output_metrics\n",
      "    stateful_metric_result = _call_stateful_fn(stateful_fn)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1773, in _call_stateful_fn\n",
      "    fn, y_true, y_pred, weights=weights, mask=mask)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 852, in call_metric_function\n",
      "    return metric_fn(y_true, y_pred, sample_weight=weights)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py\", line 438, in __call__\n",
      "    update_op = self.update_state(*args, **kwargs)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py\", line 98, in decorated\n",
      "    update_op = update_state_fn(*args, **kwargs)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/TensorFlow/1.13.1-GCCcore-8.3.0-GPU-Python-3.6.8/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py\", line 649, in update_state\n",
      "    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n",
      "  File \"<ipython-input-12-9a8ac69f7ddf>\", line 14, in f1_m\n",
      "    precision = precision_m(y_true, y_pred)\n",
      "  File \"<ipython-input-12-9a8ac69f7ddf>\", line 8, in precision_m\n",
      "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
      "NameError: name 'K' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Jupyter/2019a-gcccoremkl-8.3.0-2019.3.199-Python-3.6.8/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Jupyter/2019a-gcccoremkl-8.3.0-2019.3.199-Python-3.6.8/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Jupyter/2019a-gcccoremkl-8.3.0-2019.3.199-Python-3.6.8/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Jupyter/2019a-gcccoremkl-8.3.0-2019.3.199-Python-3.6.8/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Python/3.6.8-GCCcore-8.3.0/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Python/3.6.8-GCCcore-8.3.0/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Python/3.6.8-GCCcore-8.3.0/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Python/3.6.8-GCCcore-8.3.0/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Python/3.6.8-GCCcore-8.3.0/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/local/software/jureca/Stages/Devel-2019a/software/Python/3.6.8-GCCcore-8.3.0/lib/python3.6/posixpath.py\", line 383, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "def residual_network(x, target_size):\n",
    "    \"\"\"\n",
    "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization,\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "        y = layers.LeakyReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # resizing\n",
    "    x = layers.Lambda(lambda image: tf.image.resize(image, (target_size, target_size)))(x)\n",
    "    \n",
    "    # conv1\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "\n",
    "    # conv2\n",
    "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    for i in range(3):\n",
    "        project_shortcut = True if i == 0 else False\n",
    "        x = residual_block(x, 128, 256, _project_shortcut=project_shortcut)\n",
    "\n",
    "    # conv3\n",
    "    for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 256, 512, _strides=strides)\n",
    "\n",
    "    # conv4\n",
    "    for i in range(6):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 512, 1024, _strides=strides)\n",
    "\n",
    "    # conv5\n",
    "    for i in range(3):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 1024, 2048, _strides=strides)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(15,activation='softmax')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "\n",
    "network_output = residual_network(image_tensor, 256)\n",
    "\n",
    "model = Model(inputs=[image_tensor], outputs=[network_output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is just for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dictionary_org = {'BAS':0, 'EBO':1, 'EOS':2, 'KSC':3, 'LYA':4, 'LYT':5, 'MMZ':6, 'MOB':7, 'MON':8, 'MYB':9, 'MYO':10, 'NGB':11, 'NGS':12, 'PMB':13, 'PMO':14 }\n",
    "classes_dictionary = {value: key for key, value in classes_dictionary_org.items()}\n",
    "\n",
    "\n",
    "abbreviation_dict = { 'NGS':'Neutrophil (segmented)', \n",
    "                      'NGB':'Neutrophil (band)',\n",
    "                      'EOS':'Eosinophil',\n",
    "                      'BAS':'Basophil',\n",
    "                      'MON':'Monocyte',\n",
    "                      'LYT':'Lymphocyte (typical)',\n",
    "                      'LYA':'Lymphocyte (atypical)',\n",
    "                      'KSC':'Smudge Cell',\n",
    "                      'MYO':'Myeloblast',\n",
    "                      'PMO':'Promyelocyte',\n",
    "                      'MYB':'Myelocyte',\n",
    "                      'MMZ':'Metamyelocyte',\n",
    "                      'MOB':'Monoblast',\n",
    "                      'EBO':'Erythroblast',\n",
    "                      'PMB':'Promyelocyte (bilobed)'};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below piece of code is for manually testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"img1 = utils.load_img(image_file_path)\\nimsave('../results/input_image.png',img1)\\nimg1 = (img1[:,:,:3] *1./255)\\n\\n\\nx = image.img_to_array(img1)\\nx = np.expand_dims(x, axis=0)\\n\\nimages = np.vstack([x])\\npreds_probs = model.predict(images, batch_size=1)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''img1 = utils.load_img(image_file_path)\n",
    "imsave('../results/input_image.png',img1)\n",
    "img1 = (img1[:,:,:3] *1./255)\n",
    "\n",
    "\n",
    "x = image.img_to_array(img1)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "images = np.vstack([x])\n",
    "preds_probs = model.predict(images, batch_size=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Augmentions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_train = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=359,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14691 images belonging to 15 classes.\n",
      "Found 3674 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "#train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation\n",
    "\n",
    "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(img_height, img_width),\n",
    "                                                           class_mode='categorical')\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                              directory=validation_dir,\n",
    "                                                              target_size=(img_height, img_width),\n",
    "                                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment last line if you want to see sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_images, _ = next(train_data_gen)\n",
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plotImages(sample_training_images[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_per_class = []\n",
    "input_foldr = train_dir\n",
    "for folder in os.listdir(input_foldr):\n",
    "    if not os.path.isfile(folder):\n",
    "            files_per_class.append(len(os.listdir(input_foldr + '/' + folder)))\n",
    "total_files = sum(files_per_class)\n",
    "class_weights = {}\n",
    "for i in range(len(files_per_class)):\n",
    "    class_weights[i] = 1 - (float(files_per_class[i]) / total_files)\n",
    "#print (class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/918 [===>..........................] - ETA: 16:34 - loss: 1.1937 - accuracy: 0.7090"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a loss vs epoch graphs for both validation and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save / load the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainHistoryDict_400', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pickle.load(open('trainHistoryDict_400','rb'))\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
